(window.webpackJsonp=window.webpackJsonp||[]).push([[8],{212:function(e,t,a){"use strict";a.r(t);var r=a(0),i=Object(r.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"reinforcement-learning"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#reinforcement-learning"}},[e._v("#")]),e._v(" Reinforcement Learning")]),e._v(" "),a("h2",{attrs:{id:"post"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#post"}},[e._v("#")]),e._v(" post")]),e._v(" "),a("p",[e._v("https://distill.pub/2019/paths-perspective-on-value-learning/")]),e._v(" "),a("h2",{attrs:{id:"tools"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tools"}},[e._v("#")]),e._v(" tools")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/facebookresearch/pyrobot",target:"_blank",rel:"noopener noreferrer"}},[e._v("pyrobot"),a("OutboundLink")],1)]),e._v(" "),a("p",[a("a",{attrs:{href:"https://sites.google.com/view/replab/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Reproducible Low-Cost Arm Benchmark"),a("OutboundLink")],1),e._v(", "),a("a",{attrs:{href:"https://arxiv.org/pdf/1905.07447.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("report"),a("OutboundLink")],1)]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/mgbellemare/Arcade-Learning-Environment",target:"_blank",rel:"noopener noreferrer"}},[e._v("ALE"),a("OutboundLink")],1)]),e._v(" "),a("h2",{attrs:{id:"datasets"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#datasets"}},[e._v("#")]),e._v(" datasets")]),e._v(" "),a("h2",{attrs:{id:"workshops-confs"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#workshops-confs"}},[e._v("#")]),e._v(" workshops confs")]),e._v(" "),a("p",[e._v("http://spirl.info/2019/readings-compiled/")]),e._v(" "),a("p",[e._v("RSS 2019")]),e._v(" "),a("h2",{attrs:{id:"exploration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#exploration"}},[e._v("#")]),e._v(" exploration")]),e._v(" "),a("p",[e._v("Montezuma's revenge and pitfall")]),e._v(" "),a("blockquote",[a("p",[a("a",{attrs:{href:"https://arxiv.org/pdf/1901.10995.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Go explore"),a("OutboundLink")],1)])]),e._v(" "),a("ul",[a("li",[a("ol",[a("li",[e._v("remember states that havepreviously been visited")])])]),e._v(" "),a("li",[a("ol",{attrs:{start:"2"}},[a("li",[e._v("first return to a promising state (without exploration),then explore from it")])])]),e._v(" "),a("li",[a("ol",{attrs:{start:"3"}},[a("li",[e._v("solve simulated environments through exploiting any available means (including by introducing determinism), then robustify (create a policy that can reliably perform the solution) via imitation learning.")])])])]),e._v(" "),a("p",[a("a",{attrs:{href:"https://www.alexirpan.com/2018/11/27/go-explore.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("critic"),a("OutboundLink")],1)]),e._v(" "),a("h2",{attrs:{id:"randomization-exploration-valuef"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#randomization-exploration-valuef"}},[e._v("#")]),e._v(" Randomization(exploration, valueF)")]),e._v(" "),a("p",[e._v("https://iosband.github.io/research.html")]),e._v(" "),a("h2",{attrs:{id:"initialization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#initialization"}},[e._v("#")]),e._v(" Initialization")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://arxiv.org/pdf/1703.02660.pdf",target:"_blank",rel:"noopener noreferrer"}},[a("OutboundLink")],1),e._v(" for the MuJoCo benchmarks, wider state initialization give you more gains than pretty much any change between RL algorithms and model architectures")]),e._v(" "),a("h2",{attrs:{id:"simulation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#simulation"}},[e._v("#")]),e._v(" simulation")]),e._v(" "),a("p",[e._v("mujoco")]),e._v(" "),a("p",[e._v("https://www.panda3d.org/")]),e._v(" "),a("p",[e._v("http://bulletphysics.org/wordpress/")]),e._v(" "),a("p",[e._v("https://developer.nvidia.com/physx-sdk")]),e._v(" "),a("p",[e._v("http://www.ode.org/")]),e._v(" "),a("p",[e._v("http://gazebosim.org/")]),e._v(" "),a("p",[e._v("ODE and Gazebo have the contact support")]),e._v(" "),a("h2",{attrs:{id:"browser"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#browser"}},[e._v("#")]),e._v(" browser")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/mrdoob/three.js/blob/master/examples/webgl_loader_stl.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("three.js stl loader"),a("OutboundLink")],1)]),e._v(" "),a("h2",{attrs:{id:"testing-env"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#testing-env"}},[e._v("#")]),e._v(" testing env")]),e._v(" "),a("p",[e._v("(openai gym)[]")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://arxiv.org/pdf/1801.00690.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("dm control"),a("OutboundLink")],1)]),e._v(" "),a("p",[e._v("Arcade Learning Environment(https://github.com/mgbellemare/Arcade-Learning-Environment)")]),e._v(" "),a("p",[e._v("Roboschool(https://github.com/openai/roboschool)")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/deepmind/lab",target:"_blank",rel:"noopener noreferrer"}},[e._v("DeepMind Lab"),a("OutboundLink")],1)]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/facebookresearch/ELF",target:"_blank",rel:"noopener noreferrer"}},[e._v("ELF"),a("OutboundLink")],1)]),e._v(" "),a("h2",{attrs:{id:"model-based"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#model-based"}},[e._v("#")]),e._v(" Model Based")]),e._v(" "),a("blockquote",[a("p",[e._v("PE-TS")])]),e._v(" "),a("p",[e._v("aleatoric (inherent system stochasticity)")]),e._v(" "),a("p",[e._v("epistemic (subjective uncertainty, due to limited data)")]),e._v(" "),a("p",[e._v("Gaussian process")]),e._v(" "),a("p",[e._v("is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed.")]),e._v(" "),a("p",[e._v("Ensembles of bootstrapped models")]),e._v(" "),a("p",[e._v("CEM: samples actions from a distribution closer to previous action samples that yielded high reward")]),e._v(" "),a("p",[e._v("Specifically, aleatoric state variance is the average variance of particles of same bootstrap,")]),e._v(" "),a("p",[e._v("whilst epistemic state variance is the variance of the average of particles of same bootstrap indexes.")]),e._v(" "),a("h2",{attrs:{id:"residual-physics"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#residual-physics"}},[e._v("#")]),e._v(" Residual physics")]),e._v(" "),a("p",[e._v("Residual policy learning")]),e._v(" "),a("h2",{attrs:{id:"physics-control"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#physics-control"}},[e._v("#")]),e._v(" Physics control")]),e._v(" "),a("p",[e._v("fully-actuated in state (q,q˙) at time t if it is able to command any instantaneous acceleration in q\nunderactuated in state (q,q˙) at time t if it is not able to command an arbitrary instantaneous acceleration in  q")]),e._v(" "),a("h2",{attrs:{id:"state-abstraction"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#state-abstraction"}},[e._v("#")]),e._v(" State abstraction")]),e._v(" "),a("blockquote",[a("p",[e._v("semi-mdp(Sutton 1999)")])]),e._v(" "),a("blockquote",[a("p",[e._v("(option-critic)[https://arxiv.org/pdf/1609.05140.pdf]")])]),e._v(" "),a("h2",{attrs:{id:"distributed"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#distributed"}},[e._v("#")]),e._v(" Distributed")]),e._v(" "),a("h2",{attrs:{id:"source-of-supervision"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#source-of-supervision"}},[e._v("#")]),e._v(" source of supervision")]),e._v(" "),a("blockquote",[a("p",[e._v("demonstration")])]),e._v(" "),a("p",[a("a",{attrs:{href:"https://arxiv.org/pdf/1807.06919.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("back-play"),a("OutboundLink")],1)]),e._v(" "),a("p",[e._v("require env to be inversible, arbitraily resetting env to any states. "),a("a",{attrs:{href:"https://storage.googleapis.com/pub-tools-public-publication-data/pdf/bb67802995f7af4c6ba948ede1acfc8756be7134.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("some env"),a("OutboundLink")],1),e._v(" don't.")]),e._v(" "),a("p",[e._v("(Learning to Select and GeneralizeStriking Movements in Robot Table Tennis)[https://www.aaai.org/ocs/index.php/FSS/FSS12/paper/viewFile/5602/5884]")]),e._v(" "),a("blockquote",[a("p",[e._v("language")])]),e._v(" "),a("p",[e._v("(https://arxiv.org/pdf/1711.00482.pdf)[Learning with latent language]")]),e._v(" "),a("blockquote",[a("p",[e._v("human preference")])]),e._v(" "),a("h2",{attrs:{id:"generalization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#generalization"}},[e._v("#")]),e._v(" Generalization")]),e._v(" "),a("blockquote",[a("p",[e._v("Assessing Generalization in Deep Reinforcement Learning")])]),e._v(" "),a("p",[e._v("variations in environment dynamics")]),e._v(" "),a("p",[e._v("HalfCheetah under varying joint frictions.")]),e._v(" "),a("p",[e._v("Each environment has three versions:\ndefault; random; extreme")]),e._v(" "),a("p",[e._v("EPOpt trains an agent to be robust to environment variations by maximizing a risk-sensitive reward\nRL2 aims to learn a policy that can adapt to the environment at hand using the observed trajectory")]),e._v(" "),a("blockquote",[a("p",[e._v("A Dissection of Overfitting and Generalization inContinuous Reinforcement Learning")])]),e._v(" "),a("p",[e._v("https://arxiv.org/pdf/1806.07937.pdf")]),e._v(" "),a("p",[e._v("how to define and diagnose overfitting inMDPs, and how to reduce risks by injecting sufficient training diversity")]),e._v(" "),a("p",[e._v("tasks that received observations from natural images and explore generalization in that setting as well")]),e._v(" "),a("p",[e._v("as soon as there is enough training data diversity in thesimulated environment, deep RL generalizes well")]),e._v(" "),a("p",[e._v("deepRL algorithms show more prominent overfitting when observing natural data.")]),e._v(" "),a("p",[e._v("Results suggest that explicitly learning the dynamics model compounds existing bias in the datain the limited training seed regime")]),e._v(" "),a("p",[e._v("a methodology for detecting overfitting")]),e._v(" "),a("p",[e._v("eval-uation metrics for within-task and out-of-task generalization, consider two mechanisms for injecting noise intothe domain")]),e._v(" "),a("pre",[a("code",[e._v("- an expansion of the initial state distribution, which we implementby applying a multiplier to the initial state chosen.\n\n- Second, we evaluate policy robustness by adding Gaussian noise n ∼ N(0,σ2) directly to theobservation space\n")])]),e._v(" "),a("p",[e._v("training random seeds")]),e._v(" "),a("p",[e._v("generalization error: empirical error difference between test and training")]),e._v(" "),a("blockquote",[a("p",[e._v("https://arxiv.org/abs/1806.10729")])]),e._v(" "),a("p",[e._v("procedural generation of video game levels during training to improve generalization to human-designed levels at test time")]),e._v(" "),a("blockquote",[a("p",[e._v("safety in grid world")])]),e._v(" "),a("p",[e._v("https://deepmind.com/blog/specifying-ai-safety-problems/")]),e._v(" "),a("p",[e._v("variations in environment dynamics")]),e._v(" "),a("blockquote",[a("p",[e._v("https://www.alexirpan.com/2018/02/14/rl-hard.html")])]),e._v(" "),a("blockquote",[a("p",[e._v("https://www.alexirpan.com/2017/06/27/hyperparam-spectral.html")])]),e._v(" "),a("blockquote",[a("p",[e._v("compare gym and dm environment")])]),e._v(" "),a("ul",[a("li",[e._v("reward function design")]),e._v(" "),a("li",[e._v("does model fit to reward function design?")]),e._v(" "),a("li",[e._v("change component behavior")]),e._v(" "),a("li",[e._v("hidden instability")])]),e._v(" "),a("h1",{attrs:{id:"underactuated-robotics"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#underactuated-robotics"}},[e._v("#")]),e._v(" Underactuated Robotics")]),e._v(" "),a("p",[e._v("http://underactuated.csail.mit.edu/underactuated.html?chapter=intro")]),e._v(" "),a("blockquote",[a("p",[e._v("Quantile Regression Q learning")])]),e._v(" "),a("p",[e._v("https://arxiv.org/abs/1710.10044")]),e._v(" "),a("p",[e._v("https://mtomassoli.github.io/2017/12/08/distributional_rl/")]),e._v(" "),a("p",[e._v("quantile Q(s,a) to atoms = \\sum p_i x_i")]),e._v(" "),a("p",[e._v("sample r, s' from replay buffer, then sample from r + \\gamma Z(s', a_\\star), quantile atoms to equidistant grids, compute cross-entropy loss")]),e._v(" "),a("p",[a("strong",[e._v("unify cross-entropy and KL divergence:")])]),e._v(" "),a("p",[e._v("m is the prob of aligned atoms of $$ r + \\gamma Z(x_{x+1}, a^{\\star}) $$, and true prob $p(x_t, a_t; \\theta)$ is aligned atoms of Z(x_t,a)")]),e._v(" "),a("p",[e._v("derivatives of KL(m | p_{\\theta}) wrt. \\theta is derivative of entropy H(m, p_{\\theta}), which is the gradient of cross entropy loss function $\\sum m_i \\log p_i(x_t, a_t; \\theta)$")]),e._v(" "),a("blockquote",[a("p",[e._v("A Distributional Perspective on Reinforcement Learning")])]),e._v(" "),a("p",[e._v("fixed quantile -> variable length gaps")]),e._v(" "),a("p",[e._v("slice to N equal mass, put atoms at median")]),e._v(" "),a("p",[e._v("Huber loss for computing quantile gradient")]),e._v(" "),a("p",[a("strong",[e._v("Wasserstein metric")])]),e._v(" "),a("p",[e._v("$$\\mathcal{W}"),a("em",[e._v("{p}(X,Y)=\\left(\\int")]),e._v("{0}^{1}\\left|F_{X}^{-1}(u)-F_{Y}^{-1}(u)\\right|^{p}du\\right)^{1/p}$$")]),e._v(" "),a("p",[e._v("integrate discrepancy region, different between CDF")]),e._v(" "),a("p",[e._v("W distance is reduced when medians are aligned")]),e._v(" "),a("p",[e._v("why not simple regression:  the expectation of the quantiles are not the quantiles of the expectation")]),e._v(" "),a("blockquote",[a("p",[e._v("tune net")])]),e._v(" "),a("p",[e._v("tune the simulation physics from physics in real world")]),e._v(" "),a("blockquote",[a("p",[e._v("human correction of pose keyframes")])]),e._v(" "),a("p",[e._v("correction matrix to transform trajactories")]),e._v(" "),a("blockquote",[a("p",[e._v("diligent robot\nhospital service robot")])]),e._v(" "),a("h2",{attrs:{id:"_3d-pose-mesh-kinematic-simulation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3d-pose-mesh-kinematic-simulation"}},[e._v("#")]),e._v(" 3d pose, mesh, kinematic simulation")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/xbpeng/DeepMimic",target:"_blank",rel:"noopener noreferrer"}},[e._v("deepmimic"),a("OutboundLink")],1)]),e._v(" "),a("h2",{attrs:{id:"sample-efficiency"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sample-efficiency"}},[e._v("#")]),e._v(" sample  efficiency")]),e._v(" "),a("p",[e._v("reward  shaping\nbehavioral cloning\nreverse curriculum generation")]),e._v(" "),a("h2",{attrs:{id:"others"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#others"}},[e._v("#")]),e._v(" others")]),e._v(" "),a("blockquote",[a("p",[e._v("sticky actions "),a("a",{attrs:{href:"https://arxiv.org/pdf/1709.06009.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("section 5.2"),a("OutboundLink")],1)])]),e._v(" "),a("blockquote",[a("p",[e._v("Deep Reinforcement Learning that Matters")])]),e._v(" "),a("blockquote",[a("p",[a("a",{attrs:{href:"https://arxiv.org/abs/1803.07635",target:"_blank",rel:"noopener noreferrer"}},[e._v("motion plan and NN"),a("OutboundLink")],1)])])])}),[],!1,null,null,null);t.default=i.exports}}]);