<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Machine Learning | Josherich logs</title>
    <meta name="description" content="Josherich logs">
    <link rel="icon" href="/logs/logo.png">
  <link rel="manifest" href="/logs/manifest.json">
  <meta name="theme-color" content="#fff">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <link rel="apple-touch-icon" href="/logs/apple-touch-icon.png">
  <link rel="mask-icon" href="/logs/safari-pinned-tab.svg" color="#ff8549">
    
    <link rel="preload" href="/logs/assets/css/0.styles.7191ec3a.css" as="style"><link rel="preload" href="/logs/assets/js/app.09a71387.js" as="script"><link rel="preload" href="/logs/assets/js/2.5fba0fc3.js" as="script"><link rel="preload" href="/logs/assets/js/20.6ad277bf.js" as="script"><link rel="prefetch" href="/logs/assets/js/10.d006e7a8.js"><link rel="prefetch" href="/logs/assets/js/11.fd749eab.js"><link rel="prefetch" href="/logs/assets/js/12.e197d713.js"><link rel="prefetch" href="/logs/assets/js/13.5faee3eb.js"><link rel="prefetch" href="/logs/assets/js/14.dd1edca1.js"><link rel="prefetch" href="/logs/assets/js/15.8ccfa7a9.js"><link rel="prefetch" href="/logs/assets/js/16.0254efc1.js"><link rel="prefetch" href="/logs/assets/js/17.e177a4da.js"><link rel="prefetch" href="/logs/assets/js/18.1cf5c84a.js"><link rel="prefetch" href="/logs/assets/js/19.0a4d2d26.js"><link rel="prefetch" href="/logs/assets/js/21.f653a922.js"><link rel="prefetch" href="/logs/assets/js/22.d66a1538.js"><link rel="prefetch" href="/logs/assets/js/23.c2b61054.js"><link rel="prefetch" href="/logs/assets/js/24.a688e03c.js"><link rel="prefetch" href="/logs/assets/js/25.ce0b2b01.js"><link rel="prefetch" href="/logs/assets/js/26.98393859.js"><link rel="prefetch" href="/logs/assets/js/27.9c56208f.js"><link rel="prefetch" href="/logs/assets/js/28.2b361e5f.js"><link rel="prefetch" href="/logs/assets/js/3.e79b47f3.js"><link rel="prefetch" href="/logs/assets/js/4.824b2f94.js"><link rel="prefetch" href="/logs/assets/js/5.cf7ef866.js"><link rel="prefetch" href="/logs/assets/js/6.d8e82984.js"><link rel="prefetch" href="/logs/assets/js/7.99d61cc4.js"><link rel="prefetch" href="/logs/assets/js/8.1099fc02.js"><link rel="prefetch" href="/logs/assets/js/9.64f7f7fe.js">
    <link rel="stylesheet" href="/logs/assets/css/0.styles.7191ec3a.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/logs/" class="home-link router-link-active"><!----> <span class="site-name">Josherich logs</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/logs/" class="nav-link">Home</a></div> <a href="https://github.com/josherich/blog" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/logs/" class="nav-link">Home</a></div> <a href="https://github.com/josherich/blog" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Home</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>Logs</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/logs/logs.html" class="sidebar-link">Logs</a></li><li><a href="/logs/cloud.html" class="sidebar-link">cloud</a></li><li><a href="/logs/cpp.html" class="sidebar-link">CPP</a></li><li><a href="/logs/cs.html" class="sidebar-link">CS</a></li><li><a href="/logs/database.html" class="sidebar-link">Database</a></li><li><a href="/logs/draw.html" class="sidebar-link">Draw</a></li><li><a href="/logs/frontend.html" class="sidebar-link">Front end</a></li><li><a href="/logs/gaming.html" class="sidebar-link">Books</a></li><li><a href="/logs/josherich.html" class="sidebar-link">Josherich</a></li><li><a href="/logs/languages.html" class="sidebar-link">Language</a></li><li><a href="/logs/math.html" class="sidebar-link">Math</a></li><li><a href="/logs/ml.html" class="active sidebar-link">Machine Learning</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/logs/ml.html#recent-list" class="sidebar-link">recent list</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#tools" class="sidebar-link">Tools</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#examples-of-learning-tasks" class="sidebar-link">Examples of Learning Tasks</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#some-broad-tasks" class="sidebar-link">some broad tasks</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#general-objective" class="sidebar-link">General Objective</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#topics" class="sidebar-link">Topics</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#convexity" class="sidebar-link">Convexity</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#convex-form" class="sidebar-link">convex form</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#level-set" class="sidebar-link">level set</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#affine-set" class="sidebar-link">affine set</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#convex-hull" class="sidebar-link">convex hull</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#convex-combination" class="sidebar-link">convex combination</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#hyperlane" class="sidebar-link">hyperlane</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#halfspace" class="sidebar-link">halfspace</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#constrained-empirical-risk-minimization-ivanov" class="sidebar-link">Constrained Empirical Risk Minimization (Ivanov)</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#pennalized-empirical-risk-minimization-tikhonov" class="sidebar-link">Pennalized Empirical Risk Minimization (Tikhonov)</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#ridge-regression" class="sidebar-link">Ridge Regression</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#lasso-regression" class="sidebar-link">Lasso Regression</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#elastic-net" class="sidebar-link">Elastic Net</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#regularization-paths" class="sidebar-link">Regularization Paths</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#maximum-likelihood-estimation" class="sidebar-link">Maximum Likelihood Estimation</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#bayesian-dicision" class="sidebar-link">Bayesian dicision</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#bayesian-network" class="sidebar-link">Bayesian network</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#em" class="sidebar-link">EM</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#categorical-feature" class="sidebar-link">categorical feature</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#entropy-gain-in-tree-spliting" class="sidebar-link">entropy gain in tree spliting</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#boosting" class="sidebar-link">Boosting</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/logs/ml.html#adaboost" class="sidebar-link">AdaBoost</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#boosting-tree" class="sidebar-link">Boosting Tree</a></li></ul></li><li class="sidebar-sub-header"><a href="/logs/ml.html#definition" class="sidebar-link">Definition</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#minibatch-descent" class="sidebar-link">minibatch descent</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#sgd" class="sidebar-link">SGD</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#functional-margin" class="sidebar-link">Functional margin</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#loss-function" class="sidebar-link">Loss function</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/logs/ml.html#the-kernel-function" class="sidebar-link">The Kernel Function</a></li></ul></li><li class="sidebar-sub-header"><a href="/logs/ml.html#kernelization" class="sidebar-link">Kernelization</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#complementary-slackness" class="sidebar-link">complementary slackness</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#memm" class="sidebar-link">MEMM</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#hamming-loss" class="sidebar-link">Hamming loss</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#shatter-function" class="sidebar-link">shatter function</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#bayes-pac-hoeffding-bound" class="sidebar-link">Bayes-PAC-Hoeffding Bound</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#hessian-lipschitz" class="sidebar-link">Hessian Lipschitz</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#hoeffding’s-inequality" class="sidebar-link">Hoeffding’s inequality</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#gan" class="sidebar-link">GAN</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#wgan" class="sidebar-link">WGAN</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#vae" class="sidebar-link">VAE</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#resnet" class="sidebar-link">ResNet</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#densenet" class="sidebar-link">DenseNet</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#lambda-calculus" class="sidebar-link">lambda calculus</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#group" class="sidebar-link">group</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#bayesian-decision" class="sidebar-link">Bayesian decision</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#bayesian-action" class="sidebar-link">Bayesian action</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#james-stein-estimator" class="sidebar-link">James-Stein Estimator</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#tweedie’s-formula" class="sidebar-link">Tweedie’s formula</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/logs/ml.html#chi-squared-distribution" class="sidebar-link">Chi-squared distribution</a></li></ul></li><li class="sidebar-sub-header"><a href="/logs/ml.html#exponential-family" class="sidebar-link">exponential family</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#conjugate" class="sidebar-link">conjugate</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#maximum-a-posteriori" class="sidebar-link">maximum a posteriori</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#frequentist-statistics" class="sidebar-link">frequentist statistics</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#credible-set" class="sidebar-link">credible set</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#evaluation" class="sidebar-link">evaluation</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#relu" class="sidebar-link">ReLU</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#maxout" class="sidebar-link">maxout</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#sigmoid" class="sidebar-link">sigmoid</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#tanh" class="sidebar-link">tanh</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#icml" class="sidebar-link">ICML</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#nips" class="sidebar-link">NIPS</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#colt" class="sidebar-link">COLT</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#ecml" class="sidebar-link">ECML</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#acml" class="sidebar-link">ACML</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#correlation-and-independence" class="sidebar-link">correlation and independence</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#linear-independent" class="sidebar-link">linear independent</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#span" class="sidebar-link">span</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#basis" class="sidebar-link">basis</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#inner-product" class="sidebar-link">inner product</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#matrix-inner-product" class="sidebar-link">matrix inner product</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#function-inner-product" class="sidebar-link">function inner product</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#dot-product" class="sidebar-link">dot product</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#symmetric" class="sidebar-link">symmetric</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#inner-product-norm" class="sidebar-link">inner product norm</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#sample-variation" class="sidebar-link">sample variation</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#sample-standard-deviation" class="sidebar-link">sample standard deviation</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#correlation-coefficient" class="sidebar-link">correlation coefficient</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#holder-s-inequality" class="sidebar-link">holder's inequality</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#othogonality" class="sidebar-link">othogonality</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#gram-schmidt" class="sidebar-link">gram-schmidt</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#orthogonal-projection" class="sidebar-link">orthogonal projection</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#rank" class="sidebar-link">rank</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#trace" class="sidebar-link">trace</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#linear-map" class="sidebar-link">linear map</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#adjoint" class="sidebar-link">adjoint</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#conjugate-transpose" class="sidebar-link">conjugate transpose</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#range" class="sidebar-link">range</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#null-space" class="sidebar-link">Null space</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#column-space" class="sidebar-link">column space</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#row-space" class="sidebar-link">row space</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#orthogonal-matrices" class="sidebar-link">orthogonal matrices</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#reparametrization-trick" class="sidebar-link">reparametrization trick</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#mean-field-variational-inference" class="sidebar-link">Mean-field variational inference</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#encoder" class="sidebar-link">Encoder</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#decoder" class="sidebar-link">Decoder</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#inference" class="sidebar-link">Inference</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#optimal-mass-transportation-map" class="sidebar-link">Optimal Mass Transportation Map</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#brenier-potential-function" class="sidebar-link">Brenier potential function</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#gradient-projection" class="sidebar-link">Gradient Projection</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#蒙日-安培方程" class="sidebar-link">蒙日-安培方程</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#divergence" class="sidebar-link">divergence</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#homeomorphism" class="sidebar-link">homeomorphism</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#detailed-syllabi-for-lectures" class="sidebar-link">Detailed Syllabi for lectures:</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#detailed-syllabi-for-labs" class="sidebar-link">Detailed Syllabi for labs:</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#clique" class="sidebar-link">Clique</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#independent-set" class="sidebar-link">Independent set</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#ramsey-number" class="sidebar-link">Ramsey number</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#high-probability" class="sidebar-link">high probability</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/logs/ml.html#spencer-94-about-ramsey-number" class="sidebar-link">Spencer 94 about Ramsey number</a></li></ul></li><li class="sidebar-sub-header"><a href="/logs/ml.html#erdos-renyi-graph" class="sidebar-link">Erdos-Renyi graph</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#theorem-lower-bound-of-r-r" class="sidebar-link">theorem lower bound of R(r)</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#probabilistic-method-on-graph" class="sidebar-link">Probabilistic method on graph</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#best-known-lower-bound" class="sidebar-link">best known lower bound</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#erdos-hajnal-conjecture" class="sidebar-link">Erdos-Hajnal Conjecture</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#max-cut-problem" class="sidebar-link">max-cut problem</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/logs/ml.html#cut" class="sidebar-link">cut</a></li></ul></li><li class="sidebar-sub-header"><a href="/logs/ml.html#unique-game-problem" class="sidebar-link">Unique Game Problem</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/logs/ml.html#conjecture" class="sidebar-link">conjecture</a></li></ul></li><li class="sidebar-sub-header"><a href="/logs/ml.html#markov-random-field" class="sidebar-link">Markov random field</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#gibbs-distribution" class="sidebar-link">Gibbs distribution</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#potential-function" class="sidebar-link">potential function</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#lagrangian-multiplier" class="sidebar-link">Lagrangian multiplier</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#elman-network" class="sidebar-link">Elman Network</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#clockwork-rnn" class="sidebar-link">Clockwork RNN</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#f-test-and-t-test" class="sidebar-link">F test and t test</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#pearson-correlation" class="sidebar-link">Pearson correlation</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#our-world-in-data" class="sidebar-link">our world in data</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#misc" class="sidebar-link">Misc</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/logs/ml.html#pdf-and-pmf" class="sidebar-link">pdf and pmf</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#t分布，χ2分布，f分布" class="sidebar-link">t分布，χ2分布，F分布</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#pmi" class="sidebar-link">PMI</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#supervised-imitation-learning-lower-bound" class="sidebar-link">supervised Imitation learning lower bound</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#importance-sampling-for-switching-expectation" class="sidebar-link">importance sampling for switching expectation</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#supervised-adaptation" class="sidebar-link">supervised adaptation</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#feature-aug" class="sidebar-link">feature aug</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#emma-brunskill" class="sidebar-link">Emma Brunskill</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#aaron-roth" class="sidebar-link">Aaron roth</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#yaqi-duan" class="sidebar-link">yaqi duan</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#ruslan" class="sidebar-link">Ruslan</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#sham-kakade" class="sidebar-link">sham kakade</a></li></ul></li><li class="sidebar-sub-header"><a href="/logs/ml.html#encoder-decoder-gan" class="sidebar-link">encoder-decoder  GAN</a></li><li class="sidebar-sub-header"><a href="/logs/ml.html#warp-fusion-reconstruction" class="sidebar-link">warp fusion reconstruction</a></li></ul></li><li><a href="/logs/nlp.html" class="sidebar-link">NLP</a></li><li><a href="/logs/numpy.html" class="sidebar-link">Numpy</a></li><li><a href="/logs/ops.html" class="sidebar-link">Ops</a></li><li><a href="/logs/os.html" class="sidebar-link">OS</a></li><li><a href="/logs/papernotes.html" class="sidebar-link">paper reading notes</a></li><li><a href="/logs/pl.html" class="sidebar-link">PL</a></li><li><a href="/logs/R.html" class="sidebar-link">R</a></li><li><a href="/logs/RL.html" class="sidebar-link">Reinforcement Learning</a></li><li><a href="/logs/social.html" class="sidebar-link">Social</a></li><li><a href="/logs/vision.html" class="sidebar-link">Vision</a></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="machine-learning"><a href="#machine-learning" class="header-anchor">#</a> Machine Learning</h1> <h2 id="recent-list"><a href="#recent-list" class="header-anchor">#</a> recent list</h2> <h2 id="tools"><a href="#tools" class="header-anchor">#</a> Tools</h2> <p><a href="https://github.com/pytorch/botorch" target="_blank" rel="noopener noreferrer">botorch<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><a href="https://graphvite.io/" target="_blank" rel="noopener noreferrer">graphVite: graph embedding system<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><a href="https://github.com/ddbourgin/numpy-ml" target="_blank" rel="noopener noreferrer">ml implementation<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><a href="https://paperswithcode.com" target="_blank" rel="noopener noreferrer">https://paperswithcode.com<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><a href="https://github.com/pytorch/hub" target="_blank" rel="noopener noreferrer">pytorch hub<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><a href="https://colab.research.google.com/github/tensorflow/hub/blob/master/docs/tutorials/text_classification_with_tf_hub.ipynb" target="_blank" rel="noopener noreferrer">Colaboratory<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <blockquote><p><a href="https://www.cvxpy.org/examples/index.html" target="_blank" rel="noopener noreferrer">cvxpy<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></blockquote> <blockquote><p><a href="https://github.com/hardmaru/estool" target="_blank" rel="noopener noreferrer">evolution tool<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></blockquote> <blockquote><p><a href="https://nvidia.github.io/apex/" target="_blank" rel="noopener noreferrer">apex<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></blockquote> <blockquote><p><a href="https://github.com/google-research/disentanglement_lib" target="_blank" rel="noopener noreferrer">disentanglement lib<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></blockquote> <h2 id="examples-of-learning-tasks"><a href="#examples-of-learning-tasks" class="header-anchor">#</a> Examples of Learning Tasks</h2> <ul><li><p>Text</p></li> <li><p>Language</p></li> <li><p>Speech</p></li> <li><p>Image</p></li> <li><p>Games</p></li> <li><p>Unassisted control of vehicles</p></li> <li><p>Medical diagnosis, fraud detection, network intrusion</p></li></ul> <h2 id="some-broad-tasks"><a href="#some-broad-tasks" class="header-anchor">#</a> some broad tasks</h2> <ul><li><p>classification</p></li> <li><p>regression</p></li> <li><p>Ranking</p></li> <li><p>Clustering</p></li> <li><p>Dimensionality reduction</p></li></ul> <h2 id="general-objective"><a href="#general-objective" class="header-anchor">#</a> General Objective</h2> <ul><li><p>Theoretical questions:</p> <ul><li><p>what can be learned, under what conditions?</p></li> <li><p>are there learning guarantees?</p></li> <li><p>analysis of learning algorithms.</p></li></ul></li> <li><p>Algorithms:</p> <ul><li><p>more efficient and more accurate algorithms</p></li> <li><p>deal with large-scale problmes</p></li> <li><p>handle a variety of different learning problems</p></li></ul></li></ul> <h2 id="topics"><a href="#topics" class="header-anchor">#</a> Topics</h2> <ul><li><p><a href="#Convex-Optimization">Convex Optimization</a></p></li> <li><p>Probability tools, concentration inequalities.</p></li> <li><p><a href="#pac">PAC</a> learning model, <a href="#rademacher">Rademacher complexity</a>, <a href="#vcdimension">VC-dimension</a>.</p></li> <li><p><a href="#SVM">SVMs</a>, margin bounds, <a href="#kernel-methods">kernel methods</a>.</p></li> <li><p>ensemble methods, <a href="#boosting">boosting</a>.</p></li> <li><p>Logistic regression and conditional maximum entropy models.</p></li> <li><p>On-line learning, weighted majority algorithm, Perceptron algorithm, mistake bounds.</p></li> <li><p>Regression, generalization, algorithms.</p></li> <li><p>Ranking, generalization, algorithms.</p></li> <li><p><a href="#reinforcement">Reinforcement learning</a>, <a href="#mdp">MDPs</a>, <a href="#bandit">bandit</a> problems and algorithms.</p></li></ul> <h1 id="convex-optimization"><a href="#convex-optimization" class="header-anchor">#</a> Convex Optimization</h1> <ul><li><a href="#convexity">Convexity</a></li></ul> <h2 id="convexity"><a href="#convexity" class="header-anchor">#</a> Convexity</h2> <ul><li>definition: $X \subset \Bbb R^N $ is said to be convex if for any two points $x, y \in \mathrm X$ the segment $\lbrack x, y \rbrack$ lies in X:</li></ul> <p>$$ \lbrace \alpha x + (1-a)y, 0 \le \alpha \le 1 \rbrace \subseteq \mathrm X.$$</p> <ul><li><p>Definition: let X be a convex set. A function $\mathcal f: X \to \Bbb R$ is said to be convex if for all $x, y \in \mathrm X$ and $\alpha \in \lbrack 0,1 \rbrack$
$$f(\alpha x + (1 - \alpha)y) \le \alpha f(x) + (1 - \alpha)f(y)$$</p> <ul><li><p>with a strict inequality, $f$ is said to be strictly convex.</p></li> <li><p>$f$ is said to be concave when $-f$ is convex.</p></li></ul></li></ul> <h2 id="convex-form"><a href="#convex-form" class="header-anchor">#</a> convex form</h2> <h2 id="level-set"><a href="#level-set" class="header-anchor">#</a> level set</h2> <ul><li>contour set</li></ul> <h2 id="affine-set"><a href="#affine-set" class="header-anchor">#</a> affine set</h2> <h2 id="convex-hull"><a href="#convex-hull" class="header-anchor">#</a> convex hull</h2> <h2 id="convex-combination"><a href="#convex-combination" class="header-anchor">#</a> convex combination</h2> <h2 id="hyperlane"><a href="#hyperlane" class="header-anchor">#</a> hyperlane</h2> <h2 id="halfspace"><a href="#halfspace" class="header-anchor">#</a> halfspace</h2> <h1 id="feature-spaces"><a href="#feature-spaces" class="header-anchor">#</a> feature spaces</h1> <p>Very large feature spaces have two potential issues:</p> <ul><li><ol><li><a href="#overfitting">Overfitting</a></li></ol></li> <li><ol start="2"><li>Memory and computational costs</li></ol></li></ul> <blockquote><p>Overfitting we handle with regularization.</p></blockquote> <blockquote><p>“<a href="#kernel-methods">Kernel methods</a>” can (sometimes) help with memory and computational costs.</p></blockquote> <h1 id="empirical-risk-minimizer"><a href="#empirical-risk-minimizer" class="header-anchor">#</a> Empirical Risk Minimizer</h1> <h1 id="regularization"><a href="#regularization" class="header-anchor">#</a> regularization</h1> <h2 id="constrained-empirical-risk-minimization-ivanov"><a href="#constrained-empirical-risk-minimization-ivanov" class="header-anchor">#</a> Constrained Empirical Risk Minimization (Ivanov)</h2> <ul><li><p>complex measure of functions must be less than fixed r</p></li> <li><p>choose r using validation data or cross-validation</p></li> <li><p>each r corresponds to a different hypothesis spaces.</p></li></ul> <h2 id="pennalized-empirical-risk-minimization-tikhonov"><a href="#pennalized-empirical-risk-minimization-tikhonov" class="header-anchor">#</a> Pennalized Empirical Risk Minimization (Tikhonov)</h2> <ul><li><p>add complex measure with $\lambda$ to <a href="#empirical-risk-minimizer">minimizer</a></p></li> <li><p>choose $\lambda$ using validation data and cross-validation</p></li></ul> <h2 id="ridge-regression"><a href="#ridge-regression" class="header-anchor">#</a> Ridge Regression</h2> <ul><li><p>Tikhonov Form</p></li> <li><p>Ivanov Form</p></li> <li><p>For identical features, l2 regularization spreads weights evenly</p></li> <li><p>For linear related features, l2 regularization prefer viarables with larger scale - spreads weight proportional to scale</p></li></ul> <h2 id="lasso-regression"><a href="#lasso-regression" class="header-anchor">#</a> Lasso Regression</h2> <ul><li><p>Tikhonov Form</p></li> <li><p>Ivanov Form</p></li> <li><p>For identical features, l1 regularization spreads weights arbitrarily</p></li> <li><p>For linear related features, l1 regularization chooses viariable with larger scale, 0 to others</p></li> <li><p>why l1(Lasso) get sparsity?</p></li> <li><p>why l1(lasso) weights distribution is unstable?</p></li></ul> <h2 id="elastic-net"><a href="#elastic-net" class="header-anchor">#</a> Elastic Net</h2> <p>combine lasso and ridge penalities.</p> <ul><li><p>with uncorrelated features, we get sparsity.</p></li> <li><p>among correlated features(same scale), we spread it evenly.</p></li></ul> <h2 id="regularization-paths"><a href="#regularization-paths" class="header-anchor">#</a> Regularization Paths</h2> <h1 id="knn"><a href="#knn" class="header-anchor">#</a> KNN</h1> <p>kd tree</p> <h1 id="bayesian"><a href="#bayesian" class="header-anchor">#</a> Bayesian</h1> <h2 id="maximum-likelihood-estimation"><a href="#maximum-likelihood-estimation" class="header-anchor">#</a> Maximum Likelihood Estimation</h2> <h2 id="bayesian-dicision"><a href="#bayesian-dicision" class="header-anchor">#</a> Bayesian dicision</h2> <h2 id="bayesian-network"><a href="#bayesian-network" class="header-anchor">#</a> Bayesian network</h2> <h2 id="em"><a href="#em" class="header-anchor">#</a> EM</h2> <h1 id="decision-tree"><a href="#decision-tree" class="header-anchor">#</a> Decision tree</h1> <ul><li><p>make a decision at each node on a single feature</p></li> <li><p>for continuous value, split in the form of $x \leq t$</p></li> <li><p>for discrete value, partition values into two groups</p></li></ul> <blockquote><p>Loss function for regression
Given the partition ${ R_1,...,R_M }$, prediction function is
$$f(x) = sum_{m=1}^{M}c_m 1(x \in R_m)$$</p></blockquote> <p>for $l_2$ loss,</p> <p>for $l_1$ loss,</p> <h2 id="categorical-feature"><a href="#categorical-feature" class="header-anchor">#</a> categorical feature</h2> <p>assign each category a number, the proportion of class 0</p> <p>find optimal split as though it were a numeric feature</p> <h2 id="entropy-gain-in-tree-spliting"><a href="#entropy-gain-in-tree-spliting" class="header-anchor">#</a> entropy gain in tree spliting</h2> <p>empirical entropy</p> <p>empirical conditional entrpy</p> <p>information gain is defined as:</p> <p>$$g(D,A) = H(D) - H(D \vert A)$$</p> <p>maximize g</p> <ul><li><p>ID3</p></li> <li><p>C4.5</p></li> <li><p>pruning</p></li> <li><p>CART</p></li></ul> <h1 id="logistic-regression"><a href="#logistic-regression" class="header-anchor">#</a> logistic regression</h1> <h1 id="ensemble-learning"><a href="#ensemble-learning" class="header-anchor">#</a> ensemble learning</h1> <h2 id="boosting"><a href="#boosting" class="header-anchor">#</a> Boosting</h2> <h3 id="adaboost"><a href="#adaboost" class="header-anchor">#</a> AdaBoost</h3> <h3 id="boosting-tree"><a href="#boosting-tree" class="header-anchor">#</a> Boosting Tree</h3> <h1 id="hmm"><a href="#hmm" class="header-anchor">#</a> HMM</h1> <h1 id="conditional-random-field"><a href="#conditional-random-field" class="header-anchor">#</a> Conditional Random Field</h1> <h1 id="quadratic-problem"><a href="#quadratic-problem" class="header-anchor">#</a> Quadratic Problem</h1> <h1 id="subgradient-descent"><a href="#subgradient-descent" class="header-anchor">#</a> Subgradient descent</h1> <h1 id="kernel-methods"><a href="#kernel-methods" class="header-anchor">#</a> Kernel Methods</h1> <h2 id="definition"><a href="#definition" class="header-anchor">#</a> Definition</h2> <p>A method is kernelized if every feature vector ψ(x) only appears inside an inner product with another feature vector ψ(x′). In particular, this applies to both the optimization problem and the prediction function.</p> <h1 id="optimization"><a href="#optimization" class="header-anchor">#</a> optimization</h1> <h2 id="minibatch-descent"><a href="#minibatch-descent" class="header-anchor">#</a> minibatch descent</h2> <h2 id="sgd"><a href="#sgd" class="header-anchor">#</a> SGD</h2> <h2 id="functional-margin"><a href="#functional-margin" class="header-anchor">#</a> Functional margin</h2> <p>$$yf(x)$$</p> <ul><li><p>a measure of how correct we are</p></li> <li><p>we want to maximize the margin</p></li> <li><p>use proper <a href="#classification-loss">classification loss</a></p></li></ul> <h2 id="loss-function"><a href="#loss-function" class="header-anchor">#</a> Loss function</h2> <ul><li><p>Hinge Loss</p></li> <li><p>Zero One Loss</p></li> <li><p>Logistic Loss</p></li></ul> <p><a href="https://arxiv.org/pdf/1901.03909.pdf" target="_blank" rel="noopener noreferrer">turn local minima to non-local<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> deep or trivial, leave it to the reader to judge</p> <h1 id="svm"><a href="#svm" class="header-anchor">#</a> SVM</h1> <ul><li><p>Hypothesis space</p></li> <li><p>$\mathcal l2$ regularization</p></li> <li><p>Loss function $\mathcal{l}(m) = max{1-m, 0}$</p></li> <li><p>solution to</p></li></ul> <p>$$min_{\substack{w \in \mathbf{R}^{d}, b \in \mathbf{R}}} \frac{1}{2}\Vert w \Vert^{2} + \frac{c}{n}\sum^{n}_{i=1}max\left( 0,1-y_i \left[ \mathsf{w^T} x_i + b \right] \right) $$</p> <ul><li><p>equivalent to a quadratic problem</p></li> <li><p>differentiable</p></li> <li><p>n + d + 1 unknowns and 2n <a href="#affine">affine</a> constraints</p></li> <li><p>can be solved by <a href="#quadratic">Quadratic solver</a></p></li></ul> <h1 id="representer-theorem"><a href="#representer-theorem" class="header-anchor">#</a> Representer theorem</h1> <ul><li>assume we have minimizer of the form</li></ul> <p>$$\mathsf w^\ast = sum_{i=1}^n \alpha_i x_i</p> <ul><li><ol><li>M is the span of the data points, w is the project of $w^\ast$ onto the span,</li></ol></li> <li><ol start="2"><li>the inner product of w and x, equals to the inner product of $w^\ast$ and x,</li></ol></li> <li><ol start="3"><li>$\Vert w |Vert \leq \Vert w_\ast \Vert$, since project reduce norm, so R(w) never increases.</li></ol></li></ul> <h1 id="kernel-functions"><a href="#kernel-functions" class="header-anchor">#</a> Kernel Functions</h1> <h3 id="the-kernel-function"><a href="#the-kernel-function" class="header-anchor">#</a> The Kernel Function</h3> <ul><li><p>Input space: $\mathbf{X}$</p></li> <li><p>Feature space: $\mathbf{H}$</p></li> <li><p>Feature map: $\mathbf{\psi} : \mathbf{X} \rightarrow \mathbf{H}$</p></li></ul> <blockquote><p>The kernel function corresponding to $\psi$ is</p></blockquote> <p>$$k\left(x,x'\right) =  \langle\psi(x),\psi(x')\rangle$$</p> <p>where $\langle \cdot, \cdot \rangle$ is the inner product associated with $\mathbf{H}$.</p> <ul><li>replace inner product with kernel, represent some <a href="#Similarity-Scores">similarity score</a>.</li></ul> <p>What are the Benefits of <a href="#kernelization">Kernelization</a>?</p> <ul><li><ol><li>Computational (e.g. when feature space dimension d larger than sample size n).</li></ol></li> <li><ol start="2"><li>Can sometimes avoid any O(d) operations, allows access to infinite-dimensional feature spaces.</li></ol></li> <li><ol start="3"><li>Allows thinking in terms of “similarity” rather than features.</li></ol></li></ul> <h2 id="kernelization"><a href="#kernelization" class="header-anchor">#</a> Kernelization</h2> <ul><li>a method is kernelized if every feature vector $\psi (x)$ only appears inside an inner product with another feature vector $\psi(x')$.</li></ul> <h1 id="kernelized-svm"><a href="#kernelized-svm" class="header-anchor">#</a> Kernelized SVM</h1> <ul><li>by computing <a href="#lagrangian-dual">Lagrangian Dual</a> Problem.</li></ul> <h2 id="complementary-slackness"><a href="#complementary-slackness" class="header-anchor">#</a> complementary slackness</h2> <h1 id="similarity-scores"><a href="#similarity-scores" class="header-anchor">#</a> Similarity Scores</h1> <p>It is often useful to think of the kernel function as a similarity score. But this is not a mathematically precise statement.</p> <h1 id="overfitting"><a href="#overfitting" class="header-anchor">#</a> Overfitting</h1> <h1 id="mdps"><a href="#mdps" class="header-anchor">#</a> MDPs</h1> <h1 id="vc-dimension"><a href="#vc-dimension" class="header-anchor">#</a> VC dimension</h1> <h1 id="boosting-2"><a href="#boosting-2" class="header-anchor">#</a> Boosting</h1> <h1 id="weak-learning"><a href="#weak-learning" class="header-anchor">#</a> Weak Learning</h1> <h1 id="nmf"><a href="#nmf" class="header-anchor">#</a> NMF</h1> <p>r(n+d)
NP-hard
reformulated</p> <h1 id="lda"><a href="#lda" class="header-anchor">#</a> LDA</h1> <p>Gamma function
Dirichlet distribution</p> <h1 id="structured-prediction"><a href="#structured-prediction" class="header-anchor">#</a> structured prediction</h1> <h1 id="hmm-2"><a href="#hmm-2" class="header-anchor">#</a> HMM</h1> <p>Viterbi algorithm (dp with max)</p> <h2 id="memm"><a href="#memm" class="header-anchor">#</a> MEMM</h2> <h1 id="crf"><a href="#crf" class="header-anchor">#</a> CRF</h1> <p>https://spaces.ac.cn/archives/4695/</p> <h2 id="hamming-loss"><a href="#hamming-loss" class="header-anchor">#</a> Hamming loss</h2> <h1 id="vc-dimension-2"><a href="#vc-dimension-2" class="header-anchor">#</a> VC dimension</h1> <p>vapnik chervonenkis
growth function: expression power of hypothesis space
dichotomy
shatter
Given a set S of examples and a concept class H, we say that S is shattered by H if for every A ⊆ S there exists some h ∈ H that labels all examples in A as positive and all examples in S \ A as negative.</p> <p>The VC-dimension of H is the size of the largest set shattered by H.</p> <h2 id="shatter-function"><a href="#shatter-function" class="header-anchor">#</a> shatter function</h2> <p>Given a set S of examples and a concept class H, let H[S] = {h ∩ S : h ∈ H}. That is, H[S] is the concept class H restricted to the set of points S. For integer n and class H, let H[n] = max|S|=n |H[S]|; this is called the growth function of H.</p> <h1 id="em-2"><a href="#em-2" class="header-anchor">#</a> EM</h1> <ul><li><p>Estimation, maximization</p></li> <li><p>likelihood is usually defined on exponential function, thus use ln() to iterate EM
to get latent variables,  compute its expectation</p></li></ul> <h1 id="pac-learnable"><a href="#pac-learnable" class="header-anchor">#</a> PAC learnable</h1> <p>efficient
properly
sample complexity m &gt;= poly(,,,)</p> <blockquote><p>Definition:</p></blockquote> <p>concept class C is weakly <a href="#">PAC-learnable</a> if there exists a (weak) learning algorithm L and   &gt; 0 such that:</p> <ul><li><p>for all $\delta &gt; 0$, for all $c \in C$ and all distributions D,
$$ {Pr \atop {S \sim D}} \lbrack R(h(s)) \leq \frac{1}{2} - \gamma \rbrack \ge 1 - \delta, $$</p></li> <li><p>for sample S of size $m = poly(\frac{1}{\delta})$ for a fixed polymonial.</p></li></ul> <h2 id="bayes-pac-hoeffding-bound"><a href="#bayes-pac-hoeffding-bound" class="header-anchor">#</a> Bayes-PAC-Hoeffding Bound</h2> <p>$$\mathbb{E}_u [L(w+u)]\leq
\mathbb{E}_u [\hat{L}(w+u)] + \frac{KL(w+u||\pi) + \log
\frac{1}{\delta}}{\eta} + \frac{\eta}{2n}$$</p> <h2 id="hessian-lipschitz"><a href="#hessian-lipschitz" class="header-anchor">#</a> Hessian Lipschitz</h2> <p>$$\forall w_1, w_2, |\nabla^2 f(w_1) - \nabla^2 f(w_2)|\leq \rho
|w_1-w_2|$$</p> <blockquote><p><a href="https://blog.einstein.ai/identifying-generalization-properties-in-neural-networks/" target="_blank" rel="noopener noreferrer">identifying-generalization-properties-in-neural-networks<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></blockquote> <ul><li>model local smoothness related to generalization property of classifier</li> <li>The sharp minimizers, which led to lack of generalization ability, are characterized by a significant number of large positive eigenvalues in loss function Hessian</li> <li>how to perturb the model, not at all directions, noise should be put along the &quot;flat&quot; directions. proportion to</li></ul> <p>$$\sigma_i \approx \frac{1}{\sqrt{\nabla^2_{i,i} \hat{L}+ \rho N_{\gamma,
\epsilon}(w_i)} },$$</p> <p>$N_{\gamma, \epsilon}(w_i)=\gamma |w_i| + \epsilon$</p> <ul><li>PAC-Bayes bound suggests we should optimize the perturbed loss instead of the true loss for a better generalization.</li></ul> <h1 id="max-entropy"><a href="#max-entropy" class="header-anchor">#</a> max entropy</h1> <p>uncertainty should be equally distributed
conditional entropy</p> <h2 id="hoeffding’s-inequality"><a href="#hoeffding’s-inequality" class="header-anchor">#</a> Hoeffding’s inequality</h2> <h2 id="gan"><a href="#gan" class="header-anchor">#</a> GAN</h2> <p>cycleGAN</p> <p>pixel to pixle</p> <h2 id="wgan"><a href="#wgan" class="header-anchor">#</a> WGAN</h2> <h2 id="vae"><a href="#vae" class="header-anchor">#</a> VAE</h2> <h2 id="resnet"><a href="#resnet" class="header-anchor">#</a> ResNet</h2> <h2 id="densenet"><a href="#densenet" class="header-anchor">#</a> DenseNet</h2> <h2 id="lambda-calculus"><a href="#lambda-calculus" class="header-anchor">#</a> lambda calculus</h2> <ul><li><p>pairs</p></li> <li><p>Church numerals</p></li> <li><p>scc</p></li> <li><p>realbool</p></li> <li><p>churchbool</p></li> <li><p>realeq</p></li> <li><p>realnat</p></li> <li><p>recursion</p></li> <li><p>fix</p></li></ul> <h2 id="group"><a href="#group" class="header-anchor">#</a> group</h2> <ul><li><p>group</p></li> <li><p>half group</p></li> <li><p>ring</p></li> <li><p>field</p></li></ul> <h2 id="bayesian-decision"><a href="#bayesian-decision" class="header-anchor">#</a> Bayesian decision</h2> <ul><li><p>prior distribution</p></li> <li><p>posterior distribution</p></li> <li><p>likelihood model</p></li></ul> <h2 id="bayesian-action"><a href="#bayesian-action" class="header-anchor">#</a> Bayesian action</h2> <ul><li>minimize posterior risk</li></ul> <h2 id="james-stein-estimator"><a href="#james-stein-estimator" class="header-anchor">#</a> James-Stein Estimator</h2> <p>shrinkage</p> <p>$$ ( 1 - frac{N - 2}{\Vert z \Vert^2}) $$</p> <h2 id="tweedie’s-formula"><a href="#tweedie’s-formula" class="header-anchor">#</a> Tweedie’s formula</h2> <h3 id="chi-squared-distribution"><a href="#chi-squared-distribution" class="header-anchor">#</a> Chi-squared distribution</h3> <h2 id="exponential-family"><a href="#exponential-family" class="header-anchor">#</a> exponential family</h2> <p>$$ h(x) = exp(\eta x − \psi(\eta))h_0(x) $$</p> <p>$\eta$ : natural parameter</p> <p>$\psi$ : cumulant generating function</p> <h2 id="conjugate"><a href="#conjugate" class="header-anchor">#</a> conjugate</h2> <h2 id="maximum-a-posteriori"><a href="#maximum-a-posteriori" class="header-anchor">#</a> maximum a posteriori</h2> <h2 id="frequentist-statistics"><a href="#frequentist-statistics" class="header-anchor">#</a> frequentist statistics</h2> <h2 id="credible-set"><a href="#credible-set" class="header-anchor">#</a> credible set</h2> <h2 id="evaluation"><a href="#evaluation" class="header-anchor">#</a> evaluation</h2> <p>training, dev, test segmentation is based on the i.i.d. assumption</p> <ul><li>precison</li></ul> <p>$$\frac{TP}{TP + FP}$$</p> <ul><li>recall</li></ul> <p>$$\frac{TP}{TP + FN}$$</p> <ul><li>F1</li></ul> <p>$$\frac{2PR}{P+R} = \frac 1 2 \left( \frac 1 P + \frac 1 R \right)$$</p> <ul><li>F Beta</li></ul> <p>$$\frac{1}{1+\beta^2} \left( \frac{1}{P} + \frac{\beta^2}{R} \right)$$</p> <h2 id="relu"><a href="#relu" class="header-anchor">#</a> ReLU</h2> <h2 id="maxout"><a href="#maxout" class="header-anchor">#</a> maxout</h2> <h2 id="sigmoid"><a href="#sigmoid" class="header-anchor">#</a> sigmoid</h2> <h2 id="tanh"><a href="#tanh" class="header-anchor">#</a> tanh</h2> <h1 id="rbf-network"><a href="#rbf-network" class="header-anchor">#</a> RBF network</h1> <h1 id="ai-conference"><a href="#ai-conference" class="header-anchor">#</a> AI Conference</h1> <h2 id="icml"><a href="#icml" class="header-anchor">#</a> ICML</h2> <h2 id="nips"><a href="#nips" class="header-anchor">#</a> NIPS</h2> <h2 id="colt"><a href="#colt" class="header-anchor">#</a> COLT</h2> <h2 id="ecml"><a href="#ecml" class="header-anchor">#</a> ECML</h2> <h2 id="acml"><a href="#acml" class="header-anchor">#</a> ACML</h2> <h2 id="correlation-and-independence"><a href="#correlation-and-independence" class="header-anchor">#</a> correlation and independence</h2> <h2 id="linear-independent"><a href="#linear-independent" class="header-anchor">#</a> linear independent</h2> <p>a set of vectors is linearly independent if there exist m scalar coefficients, which are not all equal to zero and
$$ sum_{i=1}^m \alpha_i x_i = 0 $$</p> <h2 id="span"><a href="#span" class="header-anchor">#</a> span</h2> <p>all possible linear combinations of a set of vectors</p> <p>the span of any set of vectors in $\mathcal V$ is a subspace of $\mathcal V$</p> <h2 id="basis"><a href="#basis" class="header-anchor">#</a> basis</h2> <h2 id="inner-product"><a href="#inner-product" class="header-anchor">#</a> inner product</h2> <h2 id="matrix-inner-product"><a href="#matrix-inner-product" class="header-anchor">#</a> matrix inner product</h2> <h2 id="function-inner-product"><a href="#function-inner-product" class="header-anchor">#</a> function inner product</h2> <h2 id="dot-product"><a href="#dot-product" class="header-anchor">#</a> dot product</h2> <h2 id="symmetric"><a href="#symmetric" class="header-anchor">#</a> symmetric</h2> <h2 id="inner-product-norm"><a href="#inner-product-norm" class="header-anchor">#</a> inner product norm</h2> <h2 id="sample-variation"><a href="#sample-variation" class="header-anchor">#</a> sample variation</h2> <h2 id="sample-standard-deviation"><a href="#sample-standard-deviation" class="header-anchor">#</a> sample standard deviation</h2> <h2 id="correlation-coefficient"><a href="#correlation-coefficient" class="header-anchor">#</a> correlation coefficient</h2> <h2 id="holder-s-inequality"><a href="#holder-s-inequality" class="header-anchor">#</a> holder's inequality</h2> <h2 id="othogonality"><a href="#othogonality" class="header-anchor">#</a> othogonality</h2> <h2 id="gram-schmidt"><a href="#gram-schmidt" class="header-anchor">#</a> gram-schmidt</h2> <h2 id="orthogonal-projection"><a href="#orthogonal-projection" class="header-anchor">#</a> orthogonal projection</h2> <p>closest</p> <h2 id="rank"><a href="#rank" class="header-anchor">#</a> rank</h2> <p>dim(col(A)) = dim(row(A))</p> <h2 id="trace"><a href="#trace" class="header-anchor">#</a> trace</h2> <p>sum of diagonal elements</p> <h2 id="linear-map"><a href="#linear-map" class="header-anchor">#</a> linear map</h2> <p>map vectors from vector space V to vector space R</p> <p>$A\vec{x}$ is a linear combination of the columns of $A$</p> <p>linear map can be represent by a matrix</p> <ul><li><p>when matrix is fat, it projects vectors onto a lower dimensional space</p></li> <li><p>when matrix is tall, it lifts vectors onto a higher dimensional space</p></li></ul> <h2 id="adjoint"><a href="#adjoint" class="header-anchor">#</a> adjoint</h2> <p>the adjoint of a linear map satisfies</p> <p>$$ \langle f(\vec{x}), \vec{y} \rangle_{\mathcal R} = \langle \vec{x},f^{\ast}(\vec{y})_{\mathcal V} $$</p> <h2 id="conjugate-transpose"><a href="#conjugate-transpose" class="header-anchor">#</a> conjugate transpose</h2> <p>$$(A^{\ast}_{ij} = \bar{A_ji}) 1 \leq i \leq n, 1 \leq j \leq m. $$</p> <h2 id="range"><a href="#range" class="header-anchor">#</a> range</h2> <p>$$ range(f) = \lbrace \vec{y} \vert \vec{y} = f(\vec{x}) \text{, for some } \vec{x} \in \mathcal V \rbrace $$</p> <p>the range of a matrix is the range of its associated linear map</p> <h2 id="null-space"><a href="#null-space" class="header-anchor">#</a> Null space</h2> <p>the set of vectors that are mapped to zero</p> <blockquote><p>null space of a linear map is a subspace</p></blockquote> <blockquote><p>null space is perpendicular to row space
$$ null(A) = row(A)^{\perp} $$</p></blockquote> <h2 id="column-space"><a href="#column-space" class="header-anchor">#</a> column space</h2> <p>if the column spaces of two matrix are orthogonal, then inner product is 0</p> <p>$$ \langle A, B \rangle $$</p> <hr> <p>the range is the column space</p> <p>$$range(A) = col(A)$$</p> <h2 id="row-space"><a href="#row-space" class="header-anchor">#</a> row space</h2> <h2 id="orthogonal-matrices"><a href="#orthogonal-matrices" class="header-anchor">#</a> orthogonal matrices</h2> <p>orthogonal matrix is a square matrix s.t.</p> <p>$$ U^T U = U U^T = I $$</p> <blockquote><p>the columns form an orhonormal basis</p></blockquote> <blockquote><p>orthogonal matrices change the direction of vectors, not their magnitude</p></blockquote> <h1 id="variational-autoencoder"><a href="#variational-autoencoder" class="header-anchor">#</a> Variational Autoencoder</h1> <p><a href="https://github.com/altosaar/variational-autoencoder/blob/master/vae.py" target="_blank" rel="noopener noreferrer">sample implementation<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <h2 id="reparametrization-trick"><a href="#reparametrization-trick" class="header-anchor">#</a> reparametrization trick</h2> <h2 id="mean-field-variational-inference"><a href="#mean-field-variational-inference" class="header-anchor">#</a> Mean-field variational inference</h2> <h2 id="encoder"><a href="#encoder" class="header-anchor">#</a> Encoder</h2> <p>in the neural net world, the encoder is a neural network that outputs a representation zz of data xx. In probability model terms, the inference network parametrizes the approximate posterior of the latent variables zz. The inference network outputs parameters to the distribution q(z \vert x)q(z∣x).<a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/" target="_blank" rel="noopener noreferrer">credit<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <h2 id="decoder"><a href="#decoder" class="header-anchor">#</a> Decoder</h2> <p>in deep learning, the decoder is a neural net that learns to reconstruct the data xx given a representation zz. In terms of probability models, the likelihood of the data xx given latent variables zz is parametrized by a generative network. The generative network outputs parameters to the likelihood distribution p(x \vert z)p(x∣z).<a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/" target="_blank" rel="noopener noreferrer">credit<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <h2 id="inference"><a href="#inference" class="header-anchor">#</a> Inference</h2> <p>in neural nets, inference usually means prediction of latent representations given new, never-before-seen datapoints. In probability models, inference refers to inferring the values of latent variables given observed data. <a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/" target="_blank" rel="noopener noreferrer">credit<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <h1 id="optimal-transportation"><a href="#optimal-transportation" class="header-anchor">#</a> Optimal Transportation</h1> <h2 id="optimal-mass-transportation-map"><a href="#optimal-mass-transportation-map" class="header-anchor">#</a> Optimal Mass Transportation Map</h2> <h2 id="brenier-potential-function"><a href="#brenier-potential-function" class="header-anchor">#</a> Brenier potential function</h2> <h2 id="gradient-projection"><a href="#gradient-projection" class="header-anchor">#</a> Gradient Projection</h2> <h2 id="蒙日-安培方程"><a href="#蒙日-安培方程" class="header-anchor">#</a> 蒙日-安培方程</h2> <h2 id="divergence"><a href="#divergence" class="header-anchor">#</a> divergence</h2> <h2 id="homeomorphism"><a href="#homeomorphism" class="header-anchor">#</a> homeomorphism</h2> <p>isomorphism</p> <p>Homotopy</p> <h1 id="graphs-and-networks"><a href="#graphs-and-networks" class="header-anchor">#</a> Graphs and Networks</h1> <h2 id="detailed-syllabi-for-lectures"><a href="#detailed-syllabi-for-lectures" class="header-anchor">#</a> Detailed Syllabi for lectures:</h2> <p>Jan 25: Introduction to graph theory, approximation algorithm, Max-Cut approximation. Chapter 8 on Lecture Notes.</p> <p>Feb 01: Max-Cut approximation. Lifting / SDP relaxations technique in mathematical signal processing, phase retrieval and k-means SDP.</p> <p>Feb 08: Unique Games Conjecture, Sum-of-Squares interpretation of SDP relaxation. Chapter 8 of Lecture Notes.</p> <p>Feb 15: Shannon Capacity, Lovasz Theta Function. Section 7.3.1. on Lecture Notes and &quot;On the Shannon Capacity of a Graph&quot; by Laszlo Lovasz. See also Section 6.5.3.</p> <p>Feb 22: Stochastic Block Model and Phase Transitions on graphs. Chapter 9 of Lecture Notes</p> <p>Mar 01: Recovery in the Stochastic Block Model with Semidefinite relaxations. Chapter 9 of Lecture Notes</p> <h2 id="detailed-syllabi-for-labs"><a href="#detailed-syllabi-for-labs" class="header-anchor">#</a> Detailed Syllabi for labs:</h2> <p>Jan 24: review of linear algebra and probability</p> <p>Jan 31: discussion of homework 1</p> <p>Feb 07:  graph Laplacian and Cheeger's inequality</p> <p>Feb 14:  pseudo distribution for maxcut,  derivation of primal and dual program for Maxcut, SOS4</p> <p>Feb 21:  introduction to Grothendieck inequality and a proof of an upper bound of Grothendieck constant (Krivines bound)</p> <p>Feb 28:  calculate the Lovasz theta function for n-cycle and discuss connection with Grothendieck constant on graph</p> <h2 id="clique"><a href="#clique" class="header-anchor">#</a> Clique</h2> <p>A clique of a graph G is a subset S of its nodes such that the subgraph corresponding to it is complete. In other words S is a clique if all pairs of vertices in S share an edge. The clique number c(G) of G is the size of the largest clique of G.</p> <h2 id="independent-set"><a href="#independent-set" class="header-anchor">#</a> Independent set</h2> <p>An independence set of a graph G is a subset S of its nodes such that no two nodes in S share an edge. Equivalently it is a clique of the complement graph Gc := (V, Ec). The independence number of G is simply the clique number of Sc.</p> <h2 id="ramsey-number"><a href="#ramsey-number" class="header-anchor">#</a> Ramsey number</h2> <p>A natural question is whether it is possible to have arbitrarily large graphs without cliques (and without its complement having cliques), Ramsey answer this question in the negative in 1928 [Ram28]</p> <p>It is easy to show that R(3) = 6</p> <h2 id="high-probability"><a href="#high-probability" class="header-anchor">#</a> high probability</h2> <blockquote><p>We say an event happens with high probability if its probability is ≥ 1 − n−Ω(1)</p></blockquote> <h3 id="spencer-94-about-ramsey-number"><a href="#spencer-94-about-ramsey-number" class="header-anchor">#</a> Spencer 94 about Ramsey number</h3> <blockquote><p>“Erdos asks us to imagine an alien force, vastly more powerful than us, landing on Earth and demanding the value of R(5) or they will destroy our planet. In that case, he claims, we should marshal all our computers and all our mathematicians and attempt to find the value. But suppose, instead, that they ask for R(6). In that case, he believes, we should attempt to destroy the aliens.”</p></blockquote> <h2 id="erdos-renyi-graph"><a href="#erdos-renyi-graph" class="header-anchor">#</a> Erdos-Renyi graph</h2> <p>Given n and p, the random Erdos-Renyi graph G(n,p) is a random graph on n vertices where each possible edge appears, independently, with probability p.</p> <h2 id="theorem-lower-bound-of-r-r"><a href="#theorem-lower-bound-of-r-r" class="header-anchor">#</a> theorem lower bound of R(r)</h2> <p>For every r &gt;= 2,
$$R(r) \geq 2^{\frac{r-1}{2}}$$</p> <h2 id="probabilistic-method-on-graph"><a href="#probabilistic-method-on-graph" class="header-anchor">#</a> Probabilistic method on graph</h2> <p>$$\mathbb{E} \lbrack \mathbf{X(S)} \rbrack = Prob \lbrace \text{S is a clique or independent set} \rbrace = \frac{2}{2^{\left( S \atop 2 \right) }}$$</p> <h2 id="best-known-lower-bound"><a href="#best-known-lower-bound" class="header-anchor">#</a> best known lower bound</h2> <h2 id="erdos-hajnal-conjecture"><a href="#erdos-hajnal-conjecture" class="header-anchor">#</a> Erdos-Hajnal Conjecture</h2> <p>For any finite graph H, there exists a constant $\delta H &gt; 0$ such that any graph on n nodes that does
not contain H as a subgraph (is a H-free graph) must have</p> <p>$$r(G) \geq n^{\delta^H}$$</p> <h2 id="max-cut-problem"><a href="#max-cut-problem" class="header-anchor">#</a> max-cut problem</h2> <blockquote><p>to design polynomial algorithms that, in any instance, produce guaranteed approximate solutions.</p></blockquote> <p>Given a graph G = (V, E) with non-negative weights wij on the edges, find a set S ⊂ V for which cut(S) is maximal.</p> <p>Goemans and Williamson [GW95] introduced an approximation algorithm that runs in polynomial time and has a randomized component to it, and is able to obtain a cut whose expected value is guaranteed to be no smaller than a particular constant αGW times the optimum cut. The constant αGW is referred to as the approximation ratio.</p> <h3 id="cut"><a href="#cut" class="header-anchor">#</a> cut</h3> <blockquote><p>a cut is a partition of the vertices of a graph into two disjoint subsets. Any cut determines a cut-set, the set of edges that have one endpoint in each subset of the partition.</p></blockquote> <ul><li><p>max-cut</p></li> <li><p>min-cut</p></li> <li><p>sparse-cut</p> <ul><li>The sparsest cut problem is to bipartition the vertices so as to minimize the ratio of the number of edges across the cut divided by the number of vertices in the smaller half of the partition. NP-hard, best known approximation algorithm is an O({\sqrt {\log n))) approximation due to Arora, Rao &amp; Vazirani (2009)</li></ul></li> <li><p>cut-space</p></li></ul> <h2 id="unique-game-problem"><a href="#unique-game-problem" class="header-anchor">#</a> Unique Game Problem</h2> <blockquote><p>Given a graph and a set
of k colors, and, for each edge, a matching between the colors, the goal in the unique games problem
is to color the vertices as to agree with as high of a fraction of the edge matchings as possible.</p></blockquote> <h3 id="conjecture"><a href="#conjecture" class="header-anchor">#</a> conjecture</h3> <p>For any ε &gt; 0, the problem of distinguishing whether an instance of the Unique Games Problem is such that it is possible to agree with a ≥ 1 − ε fraction of the constraints or it is not possible to even agree with a ε fraction of them, is NP-hard.</p> <h1 id="probabilistic-graphic-model"><a href="#probabilistic-graphic-model" class="header-anchor">#</a> Probabilistic Graphic Model</h1> <ul><li><p>GM = Multivariate Statistics + Structure</p></li> <li><p>It is a smart way to write/specify/compose/design exponentially-large probability distributions without paying an exponential cost, and at the same time endow the distributions with structured semantics</p></li> <li><p>It refers to a family of distributions on a set of random variables that are compatible with all the probabilistic independence propositions encoded by a graph that connects these variables</p></li></ul> <p>two types:</p> <ul><li><p>Bayesian network</p></li> <li><p>Markov random field</p></li></ul> <h2 id="markov-random-field"><a href="#markov-random-field" class="header-anchor">#</a> Markov random field</h2> <p>an undirected graphical model, or Markov random filed, represents a distribution $P(X_1, ..., X_n)$ defined by and undirected graph H, and a set of positive potential functions $\Psi_c$ associated with cliques of H, s.t.</p> <p>$$P(x_1, ..., x_n) = \frac{1}{\mathbf Z} prod_{c \in C}\Psi_c(\mathbf X_c)$$</p> <p>where Z is known as the partition function</p> <h2 id="gibbs-distribution"><a href="#gibbs-distribution" class="header-anchor">#</a> Gibbs distribution</h2> <h2 id="potential-function"><a href="#potential-function" class="header-anchor">#</a> potential function</h2> <p>contingency function of its arguments assigning &quot;pre-probabilistic&quot; score of their joint configuration.</p> <h2 id="lagrangian-multiplier"><a href="#lagrangian-multiplier" class="header-anchor">#</a> Lagrangian multiplier</h2> <h1 id="neural-network"><a href="#neural-network" class="header-anchor">#</a> Neural Network</h1> <h2 id="elman-network"><a href="#elman-network" class="header-anchor">#</a> Elman Network</h2> <p>LSTM</p> <p>GRU</p> <p>Elman network就是指现在一般说的RNN（包括LSTM、GRU等等）。一个recurrent层的输出经过时延后作为下一时刻这一层的输入的一部分，然后recurrent层的输出同时送到网络后续的层，比如最终的输入层。一个Jordan network说的是直接把整个网络最终的输出（i.e. 输出层的输出）经过时延后反馈回网络的输入层</p> <h2 id="clockwork-rnn"><a href="#clockwork-rnn" class="header-anchor">#</a> Clockwork RNN</h2> <p>隐层被分成了g个模块，每个模块的大小是k，每个模块内部是全连接的。模块j到i的recurrent仅当Ti小于Tj时才会存在。根据增长的阶段对模块进行分类，模块之间的传递在隐层之间是从右向左的，从慢的模块到快的模块。</p> <p>CW-RNN跟ＲＮＮ的主要不同之处在于在每个时间点ｔ，只有模块ｉ满足ｔ%Ti=0,才会执行，产生输出。Ｔｉ的是任意的，这篇论文取得是Ｔi=２^（ｉ－１）</p> <h2 id="f-test-and-t-test"><a href="#f-test-and-t-test" class="header-anchor">#</a> F test and t test</h2> <p>Restraint and unrestrained variables
Reject the null hypothesis
Use Degree freedom1 and df2 and critical level 0.05 to look for the value, ssr r - ssr ur / ssr ur</p> <h2 id="pearson-correlation"><a href="#pearson-correlation" class="header-anchor">#</a> Pearson correlation</h2> <h2 id="our-world-in-data"><a href="#our-world-in-data" class="header-anchor">#</a> our world in data</h2> <p>https://ourworldindata.org/</p> <h2 id="misc"><a href="#misc" class="header-anchor">#</a> Misc</h2> <h3 id="pdf-and-pmf"><a href="#pdf-and-pmf" class="header-anchor">#</a> pdf and pmf</h3> <p>for the negative log-likelihood loss, ERM and MLE are equivalent
convex opt review</p> <h3 id="t分布，χ2分布，f分布"><a href="#t分布，χ2分布，f分布" class="header-anchor">#</a> t分布，χ2分布，F分布</h3> <h3 id="pmi"><a href="#pmi" class="header-anchor">#</a> PMI</h3> <p>\text{PMI}(w_1,w_2)=\log \frac{P(w_1,w_2)}{P(w_1)P(w_2)}\tag{2}</p> <h3 id="supervised-imitation-learning-lower-bound"><a href="#supervised-imitation-learning-lower-bound" class="header-anchor">#</a> supervised Imitation learning lower bound</h3> <p>At time t=1 you’re shown a picture of either a zero or a one. You have two possible actions: press a button marked “zero” or press a button marked “one.”The “correct” thing to do at t=1 is to press the button that corresponds to the image you’ve been shown. Pressing the correct button leads t0<code>1=0; the incorrect leads to</code>1=1. Now, at time t=2 you are shown another image, again of a zero or one. The correct thing todo in this time step is the xor of (a) the number written on the picture you see right now, and (b) the correct answer from the previous time step. This holds in general for t&gt;1</p> <p>single mistake never recover</p> <p>dagger</p> <p>data aggregation</p> <p>do something unusual.We put the human expert in the car, and record their actions, but the car behaves not according to the expert’s behavior, but according to f0. That is,f0is in control of the car, and the expert is trying to steer,but the car is ignoring them and simply recording their actions as training data</p> <p><a href="http://ciml.info/dl/v0_99/ciml-v0_99-ch18.pdf" target="_blank" rel="noopener noreferrer">http://ciml.info/dl/v0_99/ciml-v0_99-ch18.pdf<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><a href="https://www.notion.so/e155f85a1cf34e78b5c4f5c134a81ea3#fcb43beeb2c949a08bdc3cd4e88189b0" target="_blank" rel="noopener noreferrer"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>russian tank problem</p> <h3 id="importance-sampling-for-switching-expectation"><a href="#importance-sampling-for-switching-expectation" class="header-anchor">#</a> importance sampling for switching expectation</h3> <p><a href="https://www.notion.so/e155f85a1cf34e78b5c4f5c134a81ea3#9ed6b9a9092f49e0b9e616bbb3d6ffdd" target="_blank" rel="noopener noreferrer"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>the classifier is being told to pay more attention to training examples that have high probability under the new distribution, and less attention to training that have low probability under the new distribution.</p> <h3 id="supervised-adaptation"><a href="#supervised-adaptation" class="header-anchor">#</a> supervised adaptation</h3> <p>when the distributions agree on the value of a feature, let them share it, but when they dis-agree, allow them to learn separately</p> <h3 id="feature-aug"><a href="#feature-aug" class="header-anchor">#</a> feature aug</h3> <p>create three ver-sions of every feature: one that’s shared (for words like “awesome”),one that’s old-distribution-specific and one that’s new-distribution-specific</p> <p>fairness</p> <h1 id="nyas-machine-learning-day-2019"><a href="#nyas-machine-learning-day-2019" class="header-anchor">#</a> NYAS Machine Learning Day 2019</h1> <h3 id="emma-brunskill"><a href="#emma-brunskill" class="header-anchor">#</a> Emma Brunskill</h3> <p>policy certificate: https://arxiv.org/pdf/1811.03056.pdf</p> <p>regret minimization, epsilon bound, tighter bound: https://arxiv.org/pdf/1901.00210.pdf</p> <h3 id="aaron-roth"><a href="#aaron-roth" class="header-anchor">#</a> Aaron roth</h3> <p>differentially private fair learning</p> <p>even with unbiased dataset, learning could still be biased</p> <p><a href="https://www.nowpublishers.com/article/DownloadSummary/TCS-042" target="_blank" rel="noopener noreferrer">https://www.nowpublishers.com/article/DownloadSummary/TCS-042<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>subgroup fairness:</p> <p><a href="https://arxiv.org/pdf/1711.05144" target="_blank" rel="noopener noreferrer"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <h3 id="yaqi-duan"><a href="#yaqi-duan" class="header-anchor">#</a> yaqi duan</h3> <p>Adaptive Low-Nonnegative-Rank Approximation for State Aggregation of Markov Chains</p> <p><a href="https://arxiv.org/abs/1810.06032" target="_blank" rel="noopener noreferrer">Adaptive Low-Nonnegative-Rank Approximation for State Aggregation of Markov Chains<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>A Finite Time Analysis of Temporal Difference Learning With Linear Function Approximation</p> <p>temporal difference learning</p> <p>Attribute-Efficient Learning of Monomials over Highly-Correlated Variables</p> <p><em>Kiran Vodrahalli</em></p> <p><a href="http://www.cs.columbia.edu/~djhsu/papers/monomial-alt.pdf" target="_blank" rel="noopener noreferrer"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>Nadav Cohen, wei hu</p> <p><a href="https://openreview.net/forum?id=SkMQg3C5K7" target="_blank" rel="noopener noreferrer">A Convergence Analysis of Gradient Descent for Deep Linear Neural...<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><a href="https://arxiv.org/pdf/1901.08584.pdf" target="_blank" rel="noopener noreferrer"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>deep linear NN, close form solution, provably converge</p> <p>two condition: 1. deficiency margin c, 2. delta-balance</p> <p>std of initialization has a sweet spot for convergence</p> <p><a href="https://arxiv.org/pdf/1806.00900.pdf" target="_blank" rel="noopener noreferrer"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><a href="http://www.cohennadav.com/" target="_blank" rel="noopener noreferrer">Nadav Cohen<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks</p> <p><a href="https://www.cs.princeton.edu/~huwei/" target="_blank" rel="noopener noreferrer">Wei Hu<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>a way to compute data complexity, without training, related to intrinsic dimension</p> <h3 id="ruslan"><a href="#ruslan" class="header-anchor">#</a> Ruslan</h3> <p>Deep Generative Models with Learnable Knowledge Constraints</p> <p>teacher-student learning, build loss object with external knowledge, as extrinsic reward in RL</p> <p>use Posterior regularization to compute confidence on teacher distillation, reject samples with low confidence</p> <p>graph convolution</p> <p><a href="https://arxiv.org/pdf/1806.09764.pdf" target="_blank" rel="noopener noreferrer"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <h3 id="sham-kakade"><a href="#sham-kakade" class="header-anchor">#</a> sham kakade</h3> <p>Provably Efficient Maximum Entropy Exploration</p> <p><a href="https://scholar.google.com/citations?hl=zh-CN&amp;user=wb-DKCIAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" target="_blank" rel="noopener noreferrer"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><a href="https://arxiv.org/pdf/1812.02690.pdf" target="_blank" rel="noopener noreferrer"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>maximize cross entropy with uniform distribution, to explore</p> <p>a efficient appro plan, or density estimaiton algo, return a policy no less effective than optimal with epsilon</p> <p>the result of the estimation is as close as to the truth distribution, given the uniform as estimation prior.</p> <p>algo for unknown MDP</p> <p>Approximate planning oracle</p> <p>State distribution estimate oracle</p> <h1 id="audio"><a href="#audio" class="header-anchor">#</a> Audio</h1> <p>https://github.com/facebookresearch/wav2letter/blob/master/docs/installation.md
https://github.com/erikd/libsndfile#hacking
https://github.com/kpu/kenlm#compiling</p> <blockquote><p>feature-wise transformation
https://distill.pub/2018/feature-wise-transformations/</p></blockquote> <h1 id="pytorch"><a href="#pytorch" class="header-anchor">#</a> Pytorch</h1> <blockquote><p>sparse tensor</p></blockquote> <p><a href="https://github.com/rusty1s/pytorch_sparse/blob/wmaster/README.md" target="_blank" rel="noopener noreferrer">pytorch_sparse<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><a href="https://pytorch.org/docs/stable/sparse.html" target="_blank" rel="noopener noreferrer">docs<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><a href="https://github.com/pytorch/pytorch/issues/10043" target="_blank" rel="noopener noreferrer">use cases<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><a href="https://github.com/pytorch/pytorch/issues/8853" target="_blank" rel="noopener noreferrer">todo funs<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><a href="https://github.com/pytorch/pytorch/issues/9674" target="_blank" rel="noopener noreferrer">states<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><a href="https://github.com/pytorch/pytorch/issues/1369" target="_blank" rel="noopener noreferrer">sparse math operations<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <blockquote><p>sparse tensor in tensorflow</p></blockquote> <div class="language-python extra-class"><pre class="language-python"><code>tf<span class="token punctuation">.</span>SparseTensor<span class="token punctuation">(</span>indices<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>fm<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>fm<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>T<span class="token punctuation">,</span> values<span class="token operator">=</span>fm<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dense_shape<span class="token operator">=</span><span class="token punctuation">[</span>weight_basis<span class="token punctuation">.</span>size<span class="token punctuation">,</span> total_dim<span class="token punctuation">]</span><span class="token punctuation">)</span>
tf<span class="token punctuation">.</span>SparseTensor<span class="token punctuation">(</span>indices<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> values<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dense_shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

tf<span class="token punctuation">.</span>sparse_tensor_dense_matmul<span class="token punctuation">(</span>ww<span class="token punctuation">,</span> theta_small_norm<span class="token punctuation">,</span> adjoint_a<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> adjoint_b<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre></div><div class="language-python extra-class"><pre class="language-python"><code>modules <span class="token operator">=</span> <span class="token punctuation">[</span>module <span class="token keyword">for</span> k<span class="token punctuation">,</span> module <span class="token keyword">in</span> model<span class="token punctuation">.</span>_modules<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
</code></pre></div><p><a href="https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor" target="_blank" rel="noopener noreferrer">SparseTensor<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> <a href="https://www.tensorflow.org/api_docs/python/tf/sparse/slice" target="_blank" rel="noopener noreferrer">sparse slice<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <blockquote><p>sparse matrix in Eigen</p></blockquote> <h1 id="gan-2"><a href="#gan-2" class="header-anchor">#</a> GAN</h1> <p>birthday paradox</p> <p>mode collapse</p> <h2 id="encoder-decoder-gan"><a href="#encoder-decoder-gan" class="header-anchor">#</a> encoder-decoder  GAN</h2> <blockquote><p>https://openreview.net/pdf?id=BJehNfW0-</p></blockquote> <p>BIRTHDAY  PARADOX TEST FORGANS</p> <p>the training objective can approach its optimum value even if the generated distribution has very lowsupport —in other words,  the training objective is unable to preventmode collapse.</p> <p>cannot prevent learning meaningless codes for data</p> <p>F-Integral Probability Metric, F = {all 1 Lipchitz functions}</p> <ul><li>DESIGN OF DISCRIMINATORS WITH  RESTRICTED  APPROXIMABILITY
1-on-1 NN</li></ul> <blockquote><p>https://arxiv.org/pdf/1702.07028.pdf</p></blockquote> <p>On the ability of neural nets to express distributions</p> <p><strong>Barron theorem</strong></p> <p>if a certain quantity involving the Fourier transform is small, then the function can be approximated by a neural network with one hidden layer and a small number of nodes</p> <p>a generative model can be expressed asthe composition of n Barron functions, then it can be approximated by an+ 1-layer neural network</p> <p>if a distribution is generated bya composition ofnBarron functions, then the distribution can be approximately generated by aneural network withnhidden layers.</p> <p>Bar 93: gave a upper bound for the size of the network required in terms of a Fourier criterion. He showed that a functionf can be approximated in L2 up to error ε by a 2-layer neural network with $$O(\frac{C_f^2}{\epsilon})$$ units, where Cf depends on Fourier properties of f</p> <p>the numberof parameters required to obtain a fixed error increases linearly</p> <h2 id="warp-fusion-reconstruction"><a href="#warp-fusion-reconstruction" class="header-anchor">#</a> warp fusion reconstruction</h2> <blockquote><p>dual-quaternion</p></blockquote></div> <footer class="page-edit"><div class="edit-link"><a href="https://github.com/josherich/blog/edit/master/logs/ml.md" target="_blank" rel="noopener noreferrer">在 GitHub 上编辑此页</a> <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></div> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/logs/math.html" class="prev">Math</a></span> <span class="next"><a href="/logs/nlp.html">NLP</a>
      →
    </span></p></div> </main></div><div class="global-ui"><SWUpdatePopup></SWUpdatePopup><!----></div></div>
    <script src="/logs/assets/js/app.09a71387.js" defer></script><script src="/logs/assets/js/2.5fba0fc3.js" defer></script><script src="/logs/assets/js/20.6ad277bf.js" defer></script>
  </body>
</html>
