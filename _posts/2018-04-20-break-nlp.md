---
layout: post
title:  "Breaking NLI system"
description: "a collection of text that break NLI system"
date:   2018-04-20 14:02:39
categories: nlp
tags: [adversial, nlp]
---

As [more](https://www.aclweb.org/anthology/P18-2103) and [more](https://super.gluebenchmark.com) [research](https://www.aclweb.org/anthology/C18-1198) shows,  NLI systems are extremely vulnerable to out-of-domain distribution, "human-level" performance on test sets cannot be reproduced on real-world problems. Language is hard and the hard parts are sparse in datasets, neural models have never shown good results on small real-world datasets. The false belief that neural networks solve these problems directs people to repetitive, trivial, and redundant research. It's even more dangerous that the media cover these false positive and thus the public's false perception on related topics.

## Things are embedded in language, more or less

One way to represent common sense is `triplets`, people have tried to [mine triplets from raw text](http://conceptnet.io/), build benchmark [datasets](https://www.tau-nlp.org/commonsenseqa), but these attempts are more or less half-done pieces. It is also a part of [knowledge base construction](http://deepdive.stanford.edu/kbc). With these pieces, we can construct two styles of systems, one predicts answers directly from text, one predicts using pipelines including KBC and natural language to SQL. The latter seems more promising in this moment.

## Is it too much to expect?

The previous setup of SNLI and MNLI is designed for a general intelligent system. It should be capable of understanding things beyond lexeme, syntax and semantics, even beyond compositional semantics.
I find it convenient to imagine a system giving explanations while predicting. Compositional semantics is when a system compute double negation and consider may/could,

## Maybe it's still just overfitting

I saw this interesting work on [NLP Highlight podcast](https://podcasts.apple.com/us/podcast/87-pathologies-neural-models-make-interpretation-difficult/id1235937471?i=1000436484295), which shares a great idea found in many other important works: always assuming neural network are overfitting, as they always tend to do so.
The work find a way to prove most models make predictions with hints, even just [a single word, even a single question mark](https://arxiv.org/pdf/1804.07781.pdf).

![reducing single word](/images/break-nlp-1.png)

In the process of collecting data, it is important to exclude bias in natural language, that annotators does not generate the answer text, instead of answer the question.

## enforce another bias by back translation

Using back translation, one can translate the language distribution, isolate the bias gain in these neural models trained on large datasets.

### predict on syntax

  early Pascal RTE datasets could be correctly predicted  based  on  syntax  alone  (Vanderwende and  Dolan,  2006; Vanderwende  et  al.,  2006)
  
  artifacts allow hypothesis-only model out perform most baselines (Poliak et al.,2018b)

## ablation for datasets

1. 10% random labels, full random labels.
2. shuffle the input.
3. running input gradient process. (shi et al.)[https://arxiv.org/pdf/1804.07781.pdf]
4. randomly replace content word, predicate word.
5. adversarial insertion

## Hand-picked Diagnostic set

GLUE benchmark constructs a set of test cases on basic common sense concepts, known as [Diagnostics Main](https://super.gluebenchmark.com/diagnostics#usage). It covers many 'baseline' features of intelligent system.

  - Lexical Semantics
    - Lexical Entailment
    - Morphological Negation
    - Factivity
    - Symmetry/Collectivity
    - Redundancy
    - Named Entities
    - Quantifiers
  - Predicate-Argument Structure
    - Syntactic Ambiguity
    - Prepositional Phrases
    - Core Arguments
    - Alternations
    - Ellipsis/Implicits
    - Anaphora/Coreference
    - Intersectivity
    - Restrictivity
  - Logic
    - Propositional Structure
    - Quantification
    - Monotonicity
    - Richer Logical Structure
  - Knowledge and Common sense
    - World Knowledge
    - Common Sense

However, they decide not to include these tests in the benchmark score. Somehow this makes a crazy situation that top models exceeds human performance, while get less than 50% with these Diagnostics tests:

![diagnostic main](/images/break-nlp-2.png)

## Benchmark by Transforming Sentences

### [Swap Premise and Hypothesis](https://arxiv.org/pdf/1809.02719.pdf)

One simple but effective way to verify NLI models is to swap the sentence pair. The swapped sentence could be infered as following:

  - entailment => contradiction
  - neutral => neutral or entailment
  - contradiction => contradiction

### Time and Quantity Reasoning

> These sentences are selected from RepEval2017[^5] shared tasks, under the time/quantity resoning category. Inference on these sentences are currently quite hard for neural models.


> entailment
>   - Like one, two, three, four. 
>   - Count from one to four. 

> entailment 
>   - Of those beginning prenatal care in the first trimester, Indiana mothers rank in the lower third of individuals receiving such care nationwide. 
>   - There are Indiana mothers who do not receive prenatal care in the first trimester.  

> neutral 
>   - The value of the coefficient can vary from zero (if demand is exactly the same every week) to numbers much greater than one for wildly fluctuating weekly demand. 
>   - A coefficient can indeed rise up to a hundred for wildly fluctuating weekly demand. 

> neutral 
>   - A short time later, Nawaf and Salem al Hazmi entered the same checkpoint. 
>   - Nawaf and Salem al Hazmi entered the checkpoint ten minutes later.  

> contradiction 
>   - I think uh-oh 200 pounds of mush going to be laying on the floor, I'll never get him up you know. 
>   - There is 100 pounds of mush on the floor and I can't get him up.  

> contradiction 
>   - At best, experience with different combinations of waist sizes and leg lengths for a given design allows a scheduler to aggregate the units to be made into groups of large and small sizes, which means marker-makers can achieve efficiencies near 90 percent for casual pants. 
>   - Makrer-makers can only ever achieve efficiencies of 80 percent for casual pants.  


By transforming time phrases, we can construct sentence pairs and verify models. I list ome examples [here](https://www.josherich.me/nli.html)

## More signal

[Diverse NLI](http://www.cs.jhu.edu/~apoliak1/papers/COLLECTING-DIVERSE-NLI-PROBLEMS--EMNLP-2018.pdf)
  https://github.com/decompositional-semantics-initiative/DNC/
  
[Probing What Different NLP Tasks Teach Machines about Function Word Comprehension](https://arxiv.org/abs/1904.11544)
  https://github.com/nyu-mll/jiant/tree/naacl_probingpaper/scripts/fwords

## Subtle Aspects of Language

I collect some interesting sentences here for which natural language understanding, if it works, should be able to give a sound representation, and give multiple possible representations in some cases.

I hope people could stop focusing on boring numbers.

> 他觉得就在刚才他记得昨天下午的聚会她认为自己丑（recursive）
![他觉得就在刚才他记得昨天下午的聚会她认为自己丑](/images/nlp-1.png)

> 领导：你这是什么意思？没什么意思。意思意思。（disambiguate “意思”）
![领导：你这是什么意思？没什么意思。意思意思。](/images/nlp-2.png)

> 能穿多少穿多少(夏天/冬天)

> 地铁里听到一个女孩大概是给男朋友打电话:“我已经到西直门了，你快出来往地铁站走。如果你到了，我还没到，你就等着吧。如果我到了，你还没到，你就等着吧。”（字面意思和语气）[^1]
![他觉得就在刚才他记得昨天下午的聚会她认为自己丑](/images/nlp-3.png)

> 石室诗士施氏，嗜狮，誓食十狮。氏时时适市视狮。十时，适十狮适市。是时，适施氏适市。施氏视是十狮，恃矢势，使是十狮逝世。氏拾是十狮尸，适石室。石室湿，氏使侍拭石室。石室拭，氏始试食是十狮尸。食时，始识是十狮尸，实十石狮尸。试释是事。[^2]
> - 赵元任

>  狮识豕，豕识狮。始，狮嗜舐豕；豕适。豕时侍狮食柿，狮适。[^3]
> - 隋景芳

> 江州市长江大桥参加了长江大桥的通车仪式
![他觉得就在刚才他记得昨天下午的聚会她认为自己丑](/images/nlp-4.png)

> Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo
![他觉得就在刚才他记得昨天下午的聚会她认为自己丑](/images/nlp-5.png)

> 我在北京西站南广场东
![他觉得就在刚才他记得昨天下午的聚会她认为自己丑](/images/nlp-6.png)

> 前门到了，请各位从后门下车

> James while John had had had had had had had had had had had a better effect on the teacher

> Colorless green ideas sleep furiously. [^4]


### More

[List_of_linguistic_example_sentences](https://en.wikipedia.org/wiki/List_of_linguistic_example_sentences)

[^1]: http://iiis.tsinghua.edu.cn/~xfcui/intro2cs/slides/03_nlp.pdf
[^2]: https://zh.wikipedia.org/zh-hans/%E6%96%BD%E6%B0%8F%E9%A3%9F%E7%8D%85%E5%8F%B2
[^3]: https://www.douban.com/note/47607916/
[^4]: https://en.wikipedia.org/wiki/Colorless_green_ideas_sleep_furiously
[^5]: mismatched annotation from https://repeval2017.github.io/shared/